# L√†m sao PhantomSweep c√≥ th·ªÉ nhanh ƒë·∫øn v·∫≠y?

ƒê·ªÉ ƒë·∫°t ƒë∆∞·ª£c t·ªëc ƒë·ªô nhanh h∆°n Nmap ·ªü m·ªçi quy m√¥, ch√∫ng em ƒë√£ √°p d·ª•ng **15+ k·ªπ thu·∫≠t optimization** ·ªü m·ªçi level: System, Network, Protocol, v√† Application. D∆∞·ªõi ƒë√¢y l√† code minh h·ªça

---

## üìö PH·∫¶N 1: SYSTEM LEVEL OPTIMIZATIONS

### **1. Raw Socket - B·ªè qua t·∫•t c·∫£ abstraction layers**

**V·∫•n ƒë·ªÅ v·ªõi high-level libraries c·ªßa Scapy v√† Nmap:**

```
Application
    ‚Üì
Scapy/Library (Python overhead)
    ‚Üì
System calls (context switch)
    ‚Üì
Kernel network stack
    ‚Üì
Network driver
```

**Raw socket solution:**

```python
# ‚ùå CH·∫¨M: Scapy c√≥ nhi·ªÅu overhead
from scapy.all import send, IP, ICMP
send(IP(dst=host)/ICMP())  # ~50-100 packets/sec

# ‚úÖ NHANH: Raw socket tr·ª±c ti·∫øp
sock = socket.socket(socket.AF_INET, socket.SOCK_RAW, socket.IPPROTO_ICMP)
sock.sendto(packet_bytes, (host, 0))  # ~10,000+ packets/sec
```

**T·∫°i sao nhanh g·∫•p 100x:**

-   **No Python object creation**: Bytes thay v√¨ objects
-   **No packet building overhead**: Pre-computed
-   **Direct syscall**: Kh√¥ng qua wrappers
-   **Zero-copy**: Kernel g·ª≠i tr·ª±c ti·∫øp

**Benchmark:**

```python
# Test v·ªõi 1000 packets
import time

# Method 1: Scapy
start = time.time()
for i in range(1000):
    send(IP(dst=host)/ICMP(), verbose=0)
scapy_time = time.time() - start
# Result: ~20 seconds (50 pps)

# Method 2: Raw socket
sock = socket.socket(socket.AF_INET, socket.SOCK_RAW, socket.IPPROTO_ICMP)
packet = build_icmp_packet()  # Build once
start = time.time()
for i in range(1000):
    sock.sendto(packet, (host, 0))
raw_time = time.time() - start
# Result: ~0.1 seconds (10,000 pps)

print(f"Speedup: {scapy_time / raw_time}x")  # ~200x faster!
```

---

### **2. Pre-computed Packets - Build once, use forever**

**Naive approach (Nmap style):**

```python
# ‚ùå Rebuild packet for EVERY host
for host in hosts:  # 10,000 hosts
    packet = IP(dst=host) / ICMP(id=0x1234, seq=1)
    # Allocate memory ‚Üë
    # Build IP header ‚Üë
    # Build ICMP header ‚Üë
    # Calculate checksum ‚Üë (expensive!)
    send(packet)
```

**Optimized approach:**

```python
# ‚úÖ Build ONCE, reuse for ALL hosts
class ICMPPacket:
    def __init__(self):
        # Build template packet
        self.header = struct.pack('!BBHHH',
            8,      # Type: Echo Request
            0,      # Code
            0,      # Checksum (will calculate)
            0x1234, # ID
            1       # Sequence
        )
        self.payload = b'PhantomSweep'

        # Calculate checksum ONCE
        data = self.header + self.payload
        checksum = self._checksum(data)

        # Final packet (immutable)
        self.packet_bytes = struct.pack('!BBHHH',
            8, 0, checksum, 0x1234, 1
        ) + self.payload

    def _checksum(self, data):
        """Calculate checksum ONCE"""
        total = 0
        for i in range(0, len(data), 2):
            word = (data[i] << 8) + (data[i+1] if i+1 < len(data) else 0)
            total += word
        while total >> 16:
            total = (total & 0xFFFF) + (total >> 16)
        return ~total & 0xFFFF

# Usage
template = ICMPPacket()  # Build ONCE
packet_bytes = template.packet_bytes

# Send to 10,000 hosts - NO rebuilding!
for host in hosts:
    sock.sendto(packet_bytes, (host, 0))  # Instant!
```

**Performance impact:**

```
Build 10,000 packets individually:
  malloc: 10,000 √ó 50Œºs = 500ms
  checksum: 10,000 √ó 20Œºs = 200ms
  pack: 10,000 √ó 10Œºs = 100ms
  Total: 800ms overhead

Pre-computed packet:
  malloc: 1 √ó 50Œºs = 0.05ms
  checksum: 1 √ó 20Œºs = 0.02ms
  pack: 1 √ó 10Œºs = 0.01ms
  Total: 0.08ms overhead

‚Üí Ti·∫øt ki·ªám 10,000x operations!
```

---

### **3. Buffer Size Optimization - Tr√°nh packet loss**

**Problem:**

```python
# Default socket buffer = 128KB
# Sending 10,000 packets ‚Üí 10,000 responses arrive trong 1-2s
# 10,000 packets √ó 100 bytes = 1MB data
# ‚Üí Buffer overflow ‚Üí Packets dropped ‚Üí False negatives
```

**Solution:**

```python
# ‚úÖ Increase buffer size BEFORE sending
recv_sock = socket.socket(socket.AF_INET, socket.SOCK_RAW, socket.IPPROTO_ICMP)

# Set large receive buffer (1MB)
recv_sock.setsockopt(
    socket.SOL_SOCKET,
    socket.SO_RCVBUF,
    2**20  # 1 MB
)

# Verify buffer size
actual_size = recv_sock.getsockopt(socket.SOL_SOCKET, socket.SO_RCVBUF)
print(f"Buffer size: {actual_size / 1024}KB")  # Should be 1024KB

# Now we can handle burst traffic
```

**Impact on accuracy:**

```
Test: Scan 10,000 hosts, 234 actually alive

Default buffer (128KB):
- Buffer overflow after ~1000 responses
- Lost 134 responses
- Accuracy: 100/234 = 42.7% ‚ùå

Large buffer (1MB):
- No overflow
- Received all 234 responses
- Accuracy: 234/234 = 100% ‚úÖ
```

---

## üìö PH·∫¶N 2: ASYNC I/O & CONCURRENCY

### **4. Async I/O - Non-blocking operations**

**Blocking I/O (synchronous):**

```python
# ‚ùå CH·∫¨M: M·ªói operation block thread
def scan_host(host):
    # Connect blocks 1-75 seconds!
    sock = socket.socket()
    sock.connect((host, 80))  # ‚Üê BLOCKS HERE
    sock.close()
    return "open"

# Sequential scanning
for host in hosts:  # 1000 hosts
    result = scan_host(host)  # Wait for each
# Time: 1000 √ó 1s = 1000 seconds = 16 ph√∫t!
```

**Async I/O (non-blocking):**

```python
# ‚úÖ NHANH: T·∫•t c·∫£ operations song song
async def scan_host(host):
    try:
        # Non-blocking connection v·ªõi timeout
        reader, writer = await asyncio.wait_for(
            asyncio.open_connection(host, 80),
            timeout=1.0
        )
        writer.close()
        await writer.wait_closed()
        return "open"
    except:
        return "closed"

# Parallel scanning
async def scan_all(hosts):
    # T·∫°o 1000 tasks
    tasks = [scan_host(h) for h in hosts]
    # Ch·∫°y T·∫§T C·∫¢ ƒë·ªìng th·ªùi
    results = await asyncio.gather(*tasks)
    return results

# Time: ~1 second (parallel) thay v√¨ 16 ph√∫t!
```

**How asyncio works:**

```python
# Behind the scenes
"""
Event Loop (1 thread):

Time 0.00s: Start 1000 connections (non-blocking)
Time 0.01s: Check which connections completed
Time 0.02s: Process completed, continue waiting
...
Time 1.00s: All connections done or timed out

Key: While waiting for I/O, CPU can work on other tasks
"""

# Visualization
async def demo():
    print("Task 1: Starting connection...")
    await asyncio.sleep(1)  # ‚Üê CPU does other work here!
    print("Task 1: Done!")

async def main():
    # These run CONCURRENTLY
    await asyncio.gather(
        demo(),  # Task 1
        demo(),  # Task 2 (runs at same time!)
        demo(),  # Task 3 (runs at same time!)
    )
# Total time: 1 second (not 3 seconds!)
```

---

### **5. Semaphore-based Concurrency Control**

**Problem: Too many concurrent connections**

```python
# ‚ùå BAD: Unlimited concurrency
async def scan_unlimited(hosts):  # 100,000 hosts
    tasks = [scan_host(h) for h in hosts]
    await asyncio.gather(*tasks)
    # Creates 100,000 connections simultaneously!
    # ‚Üí Out of memory
    # ‚Üí OS file descriptor limit (ulimit)
    # ‚Üí Network congestion
```

**Solution: Semaphore limiting**

```python
# ‚úÖ GOOD: Controlled concurrency
class Scanner:
    def __init__(self, max_concurrent=1000):
        # Limit to 1000 concurrent operations
        self.semaphore = asyncio.Semaphore(max_concurrent)

    async def scan_host(self, host):
        # Wait if already 1000 tasks running
        async with self.semaphore:
            # Only 1000 hosts scanned at a time
            reader, writer = await asyncio.open_connection(host, 80)
            writer.close()
            return "open"

    async def scan_all(self, hosts):  # 100,000 hosts
        tasks = [self.scan_host(h) for h in hosts]
        results = await asyncio.gather(*tasks)
        return results
        # Safe! Never exceeds 1000 concurrent

# Execution pattern:
"""
Time 0.0s: Start 1000 connections (batch 1)
Time 0.1s: 50 completed ‚Üí Start 50 new (keep at 1000)
Time 0.2s: 200 completed ‚Üí Start 200 new
...
Time 10s: All 100,000 scanned
"""
```

**Optimal concurrency tuning:**

```python
def calculate_optimal_concurrency(target_pps, avg_response_time):
    """
    Concurrency = PPS √ó Response_Time

    Example:
    - Want 10,000 pps
    - Average response time = 0.1s
    - Optimal concurrency = 10,000 √ó 0.1 = 1,000
    """
    return int(target_pps * avg_response_time)

# LAN (fast):
# 10,000 pps √ó 0.01s response = 100 concurrent

# Internet (slow):
# 1,000 pps √ó 0.1s response = 100 concurrent

# Adjust dynamically
semaphore = asyncio.Semaphore(
    calculate_optimal_concurrency(10000, 0.01)
)
```

---

### **6. Batch Processing v·ªõi Rate Limiting**

**Problem: Sending too fast**

```python
# ‚ùå Send all at once
for host in hosts:  # 10,000 hosts
    sock.sendto(packet, (host, 0))
# ‚Üí Kernel queue overflow
# ‚Üí Network congestion
# ‚Üí Packets dropped
# ‚Üí SYN flood protection triggers
```

**Solution: Batch + Rate control**

```python
async def send_with_rate_limit(sock, hosts, pps=5000):
    """
    Send packets v·ªõi controlled rate

    Args:
        pps: Packets per second (target rate)
    """
    batch_size = 100  # Send 100 packets per batch

    for i in range(0, len(hosts), batch_size):
        batch = hosts[i:i + batch_size]

        # Send entire batch (fast!)
        for host in batch:
            sock.sendto(packet, (host, 0))

        # Calculate sleep time to maintain PPS
        # Example: 100 packets at 5000 pps = 0.02 seconds
        sleep_time = batch_size / pps
        await asyncio.sleep(sleep_time)

# Execution timeline:
"""
0.000s: Send 100 packets (takes ~0.001s)
0.020s: Sleep 0.020s (rate control)
0.020s: Send 100 packets
0.040s: Sleep 0.020s
...

Effective rate = 100 / 0.020 = 5000 pps ‚úÖ
"""
```

**Adaptive rate limiting:**

```python
class AdaptiveRateLimiter:
    def __init__(self, initial_pps=5000):
        self.pps = initial_pps
        self.error_count = 0

    async def send_batch(self, sock, batch):
        try:
            for host in batch:
                sock.sendto(packet, (host, 0))

            # Success ‚Üí Increase rate
            if self.error_count == 0:
                self.pps = min(self.pps * 1.2, 10000)
            else:
                self.error_count = max(0, self.error_count - 1)

        except Exception:
            # Error ‚Üí Decrease rate
            self.error_count += 1
            self.pps = max(self.pps * 0.5, 100)

        # Dynamic sleep
        await asyncio.sleep(len(batch) / self.pps)

# Self-adjusts based on network conditions!
```

---

## üìö PH·∫¶N 3: NETWORK PROTOCOL OPTIMIZATIONS

### **7. BPF (Berkeley Packet Filter) - Kernel-level filtering**

**Without BPF (userspace filtering):**

```python
# ‚ùå All packets go to userspace
def packet_handler(pkt):
    # Python receives EVERY network packet!
    if pkt.haslayer(ICMP):
        if pkt[ICMP].type == 0:  # Echo Reply
            if pkt[IP].src in target_hosts:
                process(pkt)
    # Waste: Process 99% irrelevant packets

sniffer = AsyncSniffer(prn=packet_handler)  # No filter
# CPU: 100% processing all packets!
```

**With BPF (kernel-level filtering):**

```python
# ‚úÖ Kernel filters, only relevant packets to userspace
bpf_filter = "icmp[icmptype] == icmp-echoreply"

def packet_handler(pkt):
    # Python only receives ICMP Echo Reply packets!
    process(pkt)  # All packets are relevant

sniffer = AsyncSniffer(filter=bpf_filter, prn=packet_handler)
# CPU: 5% - kernel filtered 95% packets!
```

**BPF syntax examples:**

```python
# ICMP Echo Reply only
"icmp[icmptype] == icmp-echoreply"

# TCP with SYN or RST flags
"tcp[tcpflags] & (tcp-syn|tcp-rst) != 0"

# ICMP Port Unreachable
"icmp and icmp[icmptype] == icmp-unreach and icmp[icmpcode] == 3"

# Specific source IPs (small scan)
"src host 192.168.1.1 or src host 192.168.1.2"

# Complex filter
"(tcp dst port 80 and tcp[tcpflags] & tcp-syn != 0) or icmp"
```

**Performance impact:**

```
Test: 100,000 packets/sec network traffic
Target: 100 ICMP Echo Reply packets

Without BPF:
- Python processes: 100,000 packets/sec
- CPU usage: 80-100%
- Relevant packets found: 100

With BPF:
- Kernel filters: 99,900 packets (dropped)
- Python processes: 100 packets/sec
- CPU usage: 5%
- Relevant packets found: 100

‚Üí 1000x less CPU usage!
```

---

### **8. Checksum Optimization**

**Naive checksum (byte-by-byte):**

```python
def slow_checksum(data):
    total = 0
    # Process 1 byte at a time
    for i in range(len(data)):
        total += data[i]
    return ~total & 0xFFFF

# For 20-byte packet: 20 iterations
```

**Optimized checksum (word-by-word):**

```python
def fast_checksum(data):
    total = 0
    # Process 2 bytes (16-bit word) at a time
    for i in range(0, len(data) - 1, 2):
        word = (data[i] << 8) + data[i + 1]
        total += word

    # Handle odd byte
    if len(data) % 2:
        total += data[-1] << 8

    # Handle carry bits
    while total >> 16:
        total = (total & 0xFFFF) + (total >> 16)

    return ~total & 0xFFFF

# For 20-byte packet: 10 iterations
# ‚Üí 2x faster
```

**SIMD-optimized checksum (advanced):**

```python
import numpy as np

def simd_checksum(data):
    """
    Use SIMD instructions to process multiple words at once
    """
    # Convert to numpy array (enables SIMD)
    arr = np.frombuffer(data, dtype=np.uint16)

    # Sum all words in parallel (SIMD)
    total = np.sum(arr, dtype=np.uint32)

    # Handle carry
    while total >> 16:
        total = (total & 0xFFFF) + (total >> 16)

    return ~int(total) & 0xFFFF

# For 20-byte packet: 1 SIMD instruction
# ‚Üí 20x faster than byte-by-byte!
```

---

### **9. TCP Flags Bit Masking - Fast flag detection**

**Slow string comparison:**

```python
# ‚ùå Convert to string, compare
flags_str = str(pkt[TCP].flags)
if flags_str == "SA":  # SYN/ACK
    print("Open port")
elif flags_str == "R":  # RST
    print("Closed port")
# Slow: String allocation + comparison
```

**Fast bit masking:**

```python
# ‚úÖ Direct bit operations
flags = pkt[TCP].flags

# TCP flags byte: |URG|ACK|PSH|RST|SYN|FIN|
#                  0x20 0x10 0x08 0x04 0x02 0x01

# Check SYN/ACK (0x12 = 0x10 + 0x02)
if flags & 0x12 == 0x12:
    print("Open port")  # Instant!

# Check RST (0x04)
elif flags & 0x04:
    print("Closed port")  # Instant!

# Check SYN only
if flags & 0x02 and not flags & 0x10:
    print("SYN packet")
```

**Performance:**

```python
# Benchmark: Check 1 million packets
import time

# Method 1: String comparison
start = time.time()
for _ in range(1000000):
    flags_str = "SA"
    if flags_str == "SA":
        pass
string_time = time.time() - start
# ~0.5 seconds

# Method 2: Bit masking
start = time.time()
for _ in range(1000000):
    flags = 0x12
    if flags & 0x12 == 0x12:
        pass
bit_time = time.time() - start
# ~0.05 seconds

print(f"Speedup: {string_time / bit_time}x")  # ~10x faster
```

---

## üìö PH·∫¶N 4: SMART ALGORITHMS

### **10. Exponential Backoff Waiting**

**Problem: Wasting CPU on frequent checks**

```python
# ‚ùå Fixed polling interval
async def wait_naive(targets, timeout):
    start = time.time()
    while (time.time() - start) < timeout:
        check_completion()
        await asyncio.sleep(0.01)  # Check every 10ms
        # If timeout = 10s ‚Üí 1000 checks!

# CPU wasted on unnecessary checks
```

**Solution: Exponential backoff**

```python
# ‚úÖ Check frequently early, infrequently later
async def wait_smart(targets, timeout):
    start = time.time()
    check_interval = 0.02  # Start: 20ms
    max_interval = 0.5     # Max: 500ms

    while (time.time() - start) < timeout:
        if len(discovered) >= len(targets):
            break  # Early exit!

        await asyncio.sleep(check_interval)

        # Exponential backoff
        check_interval = min(check_interval * 1.3, max_interval)

# Execution pattern:
"""
0.00s: Check (interval=0.02s)
0.02s: Check (interval=0.026s)
0.05s: Check (interval=0.034s)
0.08s: Check (interval=0.044s)
0.13s: Check (interval=0.057s)
...
5.0s:  Check (interval=0.5s, maxed out)

Total: ~50 checks instead of 1000
‚Üí 20x less CPU usage
"""
```

**Why it works:**

```
Insight: Most responses arrive EARLY

ICMP replies distribution:
- 80% arrive in first 100ms
- 15% arrive in next 900ms
- 5% arrive in last 9 seconds

‚Üí Check frequently early (when responses come)
‚Üí Check rarely later (few responses)
```

---

### **11. Smart Timeout Calculation**

**Fixed timeout (Nmap style):**

```python
# ‚ùå One size fits all
timeout = 5.0  # Always 5 seconds
# Too short for slow networks
# Too long for fast networks
```

**Adaptive timeout:**

```python
def calculate_smart_timeout(num_targets, network_type, base_timeout):
    """
    Calculate optimal timeout based on:
    1. Number of targets (more targets = slight increase)
    2. Network type (LAN vs Internet)
    3. Base timeout from user config
    """

    # Network-based adjustment
    if network_type == 'LAN':
        multiplier = 0.5  # Fast response
    elif network_type == 'Internet':
        multiplier = 1.5  # Slower response
    else:
        multiplier = 1.0  # Default

    # Scale-based adjustment
    if num_targets <= 100:
        # Small scan - aggressive
        timeout = base_timeout * 0.5
    elif num_targets <= 1000:
        # Medium scan - normal
        timeout = base_timeout * 1.0
    else:
        # Large scan - BUT NOT linear!
        # Insight: Responses come in parallel
        timeout = base_timeout * 1.2

    # Apply network multiplier
    timeout *= multiplier

    # Clamp to reasonable range
    return max(1.0, min(30.0, timeout))

# Examples:
# LAN, 50 hosts:     1.0s (fast)
# Internet, 50 hosts: 3.0s (allow latency)
# LAN, 10000 hosts:  2.4s (not 50s!)
```

---

### **12. Early Termination**

**Wait full timeout:**

```python
# ‚ùå Always wait full timeout
async def scan_naive(targets, timeout):
    start_time = time.time()

    # Send all packets
    send_all_packets(targets)

    # Wait FULL timeout, even if done early
    await asyncio.sleep(timeout)

    return discovered

# If all responses arrive in 0.5s, still waits 10s!
```

**Early termination:**

```python
# ‚úÖ Exit as soon as all found
async def scan_smart(targets, timeout):
    start_time = time.time()

    send_all_packets(targets)

    while (time.time() - start_time) < timeout:
        # Check if all targets responded
        if len(discovered) >= len(targets):
            elapsed = time.time() - start_time
            print(f"All found in {elapsed:.2f}s, exiting early!")
            break

        await asyncio.sleep(0.05)

    return discovered

# Scan 100 hosts:
# - All respond in 0.8s
# - Exit at 0.8s (not wait 10s)
# ‚Üí 12x faster!
```

---

### **13. Connection Reuse (TCP specific)**

**Create new connection each time:**

```python
# ‚ùå Overhead of connection setup
async def check_port(host, port):
    reader, writer = await asyncio.open_connection(host, port)
    writer.close()
    await writer.wait_closed()
    # 3-way handshake for EACH port
    # Overhead: SYN, SYN/ACK, ACK, FIN, FIN/ACK
```

**Connection pooling:**

```python
# ‚úÖ Reuse connections when possible
class ConnectionPool:
    def __init__(self):
        self.connections = {}

    async def get_connection(self, host):
        if host in self.connections:
            # Reuse existing connection
            return self.connections[host]

        # Create new connection
        reader, writer = await asyncio.open_connection(host, 22)
        self.connections[host] = (reader, writer)
        return reader, writer

    async def check_port(self, host, port):
        # Get or create connection
        reader, writer = await self.get_connection(host)

        # Use connection to probe port
        # (Implementation depends on protocol)

        # Keep connection open for next port!

# Benefit: Save handshake overhead
# 1000 ports on 1 host:
# - Without pool: 1000 √ó 3-way handshake
# - With pool: 1 √ó 3-way handshake
# ‚Üí 1000x less overhead!
```

---

## üìö PH·∫¶N 5: MEMORY & DATA STRUCTURE OPTIMIZATIONS

### **14. Efficient Data Structures**

**Slow lookups:**

```python
# ‚ùå List lookup: O(n)
discovered = []  # List
if host in discovered:  # O(n) - scan entire list!
    pass

# For 10,000 hosts: worst case 10,000 comparisons
```

**Fast lookups:**

```python
# ‚úÖ Set lookup: O(1)
discovered = set()  # Hash set
if host in discovered:  # O(1) - instant!
    pass

# For 10,000 hosts: always 1 hash lookup
```

**Benchmark:**

```python
import time

# Test: Check membership 10,000 times
hosts = [f"192.168.{i//256}.{i%256}" for i in range(10000)]

# Method 1: List
discovered_list = []
for h in hosts[:5000]:
    discovered_list.append(h)

start = time.time()
for h in hosts:
    if h in discovered_list:
        pass
list_time = time.time() - start
# ~5 seconds (O(n) for each check)

# Method 2: Set
discovered_set = set()
for h in hosts[:5000]:
    discovered_set.add(h)

start = time.time()
for h in hosts:
    if h in discovered_set:
        pass
set_time = time.time() - start
# ~0.01 seconds (O(1) for each check)

print(f"Speedup: {list_time / set_time}x")  # ~500x faster!
```

---

### **15. Memory Pooling**

**Allocate each time:**

```python
# ‚ùå Allocate memory for each packet
def process_packets(count):
    for i in range(count):
        # Allocate new buffer
        buffer = bytearray(1024)
        # Use buffer
        process(buffer)
        # Buffer deallocated (GC overhead)

# For 10,000 packets: 10,000 malloc + free
```

**Reuse buffers:**

```python
# ‚úÖ Allocate once, reuse
class BufferPool:
    def __init__(self, buffer_size=1024, pool_size=100):
        # Pre-allocate buffers
        self.buffers = [bytearray(buffer_size) for _ in range(pool_size)]
        self.available = list(range(pool_size))

    def get_buffer(self):
        if self.available:
            idx = self.available.pop()
            return self.buffers[idx], idx
        # Create new if pool exhausted
        return bytearray(1024), -1

    def return_buffer(self, idx):
        if idx >= 0:
            self.available.append(idx)

# Usage
pool = BufferPool()

def process_packets(count):
    for i in range(count):
        buffer, idx = pool.get_buffer()
        process(buffer)
        pool.return_buffer(idx)  # Reuse!

# For 10,000 packets: 100 malloc (reused 100x each)
# ‚Üí 100x less memory allocation!
```

---

## üìö PH·∫¶N 6: OS & KERNEL OPTIMIZATIONS

### **16. Socket Options Tuning**

```python
# Optimize socket for performance
sock = socket.socket(socket.AF_INET, socket.SOCK_RAW, socket.IPPROTO_ICMP)

# 1. Increase send buffer
sock.setsockopt(socket.SOL_SOCKET, socket.SO_SNDBUF, 2**20)  # 1MB

# 2. Increase receive buffer
sock.setsockopt(socket.SOL_SOCKET, socket.SO_RCVBUF, 2**20)  # 1MB

# 3. Set non-blocking mode
sock.setblocking(False)

# 4. Reuse address (avoid TIME_WAIT)
sock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)

# 5. TCP-specific: Disable Nagle's algorithm
sock.setsockopt(socket.IPPROTO_TCP, socket.TCP_NODELAY, 1)

# 6. Set timeout
sock.settimeout(1.0)
```

---

### **17. Kernel Parameter Tuning**

```bash
# Increase file
```
